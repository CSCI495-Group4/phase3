{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7067c6e7",
   "metadata": {},
   "source": [
    "\n",
    "# Model A : Multimodal Emotion Recognition (ResNet18 + GRU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c46ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d171485e",
   "metadata": {},
   "source": [
    "\n",
    "# Architecture Overview\n",
    "\n",
    "### Image Backbone : ResNet-18 (pretrained) → 512-D image feature\n",
    "### Text Encoder   : GRU (Embedding + GRU)  → 512-D text embedding\n",
    "### Fusion         : Concatenate [512 + 512] → 1024-D\n",
    "### Dropout        : p = 0.5\n",
    "### Head           : Linear(1024 → 7) + Softmax\n",
    "### Loss           : CrossEntropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb601f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelA_MultimodalEmotionNet(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, hidden_dim=512, num_classes=7):\n",
    "        super(ModelA_MultimodalEmotionNet, self).__init__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560c960a",
   "metadata": {},
   "source": [
    " ###  Image Path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd039afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_resnet = models.resnet18(pretrained=True)\n",
    "        modules = list(base_resnet.children())[:-1]  # remove final classification layer\n",
    "        self.image_encoder = nn.Sequential(*modules)\n",
    "        self.image_fc = nn.Linear(512, hidden_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1dc798",
   "metadata": {},
   "source": [
    "### - Text Path -\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921d9a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "      self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f889276",
   "metadata": {},
   "source": [
    "## - Fusion + Classification -\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d1b778",
   "metadata": {},
   "outputs": [],
   "source": [
    "  self.dropout = nn.Dropout(p=0.5)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, images, text):\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a6137c",
   "metadata": {},
   "source": [
    "## Image pipeline\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597898c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_feat = self.image_encoder(images).view(images.size(0), -1)\n",
    "        img_feat = self.image_fc(img_feat)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5edc1c8",
   "metadata": {},
   "source": [
    "## Text pipeline\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414820dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded = self.embedding(text)\n",
    "        _, text_feat = self.gru(embedded)\n",
    "        text_feat = text_feat.squeeze(0)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427953d1",
   "metadata": {},
   "source": [
    "## Fusion\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34a2263",
   "metadata": {},
   "outputs": [],
   "source": [
    "fused = torch.cat((img_feat, text_feat), dim=1)\n",
    "        fused = self.dropout(fused)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240106e5",
   "metadata": {},
   "source": [
    "## Classification\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e9bc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = self.fc(fused)\n",
    "        probs = self.softmax(logits)\n",
    "        return probs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a7f24f",
   "metadata": {},
   "source": [
    "# Example Initialization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe64539b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    modelA = ModelA_MultimodalEmotionNet(vocab_size=10000, num_classes=7)\n",
    "    sample_image = torch.randn(8, 3, 224, 224)   # batch of 8 images\n",
    "    sample_text = torch.randint(0, 9999, (8, 20)) # batch of 8 text sequences\n",
    "    output = modelA(sample_image, sample_text)\n",
    "    print(\"Output shape:\", output.shape)  # expected [8, 7]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
